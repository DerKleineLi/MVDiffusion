{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import cv2\n",
    "import torch\n",
    "from einops import rearrange\n",
    "import k3d\n",
    "\n",
    "\n",
    "def transpose(R, t, X):\n",
    "    b, h, w, c = X.shape\n",
    "    X = rearrange(X, \"b h w c -> b c (h w)\")\n",
    "\n",
    "    X_after_R = R @ X + t[:, :, None]\n",
    "\n",
    "    X_after_R = rearrange(X_after_R, \"b c (h w) -> b h w c\", h=h)\n",
    "    return X_after_R\n",
    "\n",
    "\n",
    "def pi_inv(K, x, d):\n",
    "    fx, fy, cx, cy = K[:, 0:1, 0:1], K[:, 1:2, 1:2], K[:, 0:1, 2:3], K[:, 1:2, 2:3]\n",
    "    X_x = d * (x[..., 0] - cx) / fx\n",
    "    X_y = d * (x[..., 1] - cy) / fy\n",
    "    X_z = d\n",
    "\n",
    "    X = torch.stack([X_x, X_y, X_z], dim=-1)\n",
    "    return X\n",
    "\n",
    "\n",
    "def x_2d_coords(h, w, device):\n",
    "    x_2d = torch.zeros((h, w, 2), device=device)\n",
    "    for y in range(0, h):\n",
    "        x_2d[y, :, 1] = y\n",
    "    for x in range(0, w):\n",
    "        x_2d[:, x, 0] = x\n",
    "    return x_2d\n",
    "\n",
    "\n",
    "def back_projection(depth, pose, K, x_2d=None):\n",
    "    b, h, w = depth.shape\n",
    "    if x_2d is None:\n",
    "        x_2d = x_2d_coords(h, w, device=depth.device)[None, ...].repeat(b, 1, 1, 1)\n",
    "\n",
    "    X_3d = pi_inv(K, x_2d, depth)\n",
    "\n",
    "    Rwc, twc = pose[:, :3, :3], pose[:, :3, 3]\n",
    "    X_world = transpose(Rwc, twc, X_3d)\n",
    "\n",
    "    X_world = X_world.reshape((-1, h, w, 3))\n",
    "    return X_world"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_name = \"LivingRoom-36282\"\n",
    "short_prompt = \"classic\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1845801/206854593.py:21: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  K = torch.tensor([K])\n"
     ]
    }
   ],
   "source": [
    "output_dir = Path(\"outputs\") / scene_name / short_prompt / \"images\"\n",
    "depth_inv_files = sorted(output_dir.glob(\"*_depth_inv.png\"))\n",
    "depth_files = sorted(output_dir.glob(\"*_depth.png\"))\n",
    "pred_files = sorted(output_dir.glob(\"*_pred.png\"))\n",
    "pose_files = sorted(output_dir.glob(\"*_poses.txt\"))\n",
    "K_file = output_dir / \"K.txt\"\n",
    "\n",
    "K = np.loadtxt(K_file)\n",
    "poses = [np.loadtxt(f) for f in pose_files]\n",
    "# for i in range(len(poses)):\n",
    "#     poses[i][[0, 1]] *= -1\n",
    "#     poses[i] = np.linalg.inv(poses[i])\n",
    "# poses = [np.linalg.inv(p) for p in poses]\n",
    "poses = np.stack(poses, axis=0)\n",
    "depths = [cv2.imread(str(f), cv2.IMREAD_UNCHANGED) for f in depth_files]\n",
    "depths = [d.astype(np.float32) / 1000.0 for d in depths]\n",
    "depths = np.stack(depths, axis=0)\n",
    "\n",
    "depths = torch.tensor(depths)\n",
    "poses = torch.tensor(poses)\n",
    "K = torch.tensor([K])\n",
    "\n",
    "# latents = torch.cat([latents] * 2)\n",
    "# timestep = torch.cat([timestep] * 2)\n",
    "# poses = torch.cat([batch[\"poses\"]] * 2)\n",
    "# K = torch.cat([batch[\"K\"]] * 2)\n",
    "# depths = torch.cat([batch[\"depths\"]] * 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_world = back_projection(depths, poses, K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/rhome/hli/.local/lib/python3.8/site-packages/traittypes/traittypes.py:97: UserWarning: Given trait value dtype \"float64\" does not match required type \"float32\". A coerced copy has been created.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7d71a270397d4d479dbcee5180571aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "X_flat = X_world.reshape((-1, 3))\n",
    "X_flat = X_flat.cpu().numpy()\n",
    "\n",
    "plot = k3d.plot()\n",
    "points = k3d.points(X_flat, point_size=0.01)\n",
    "plot += points\n",
    "plot.display()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mvdiffusion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
